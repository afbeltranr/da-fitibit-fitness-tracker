{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Exploring the fit bit kaggle dataset \n",
    "\n",
    "This notebook aims to explore and analyze the Fitbit dataset. The dataset contains various CSV files with information on daily activities, calories burned, heart rate, sleep patterns, and more. We will start by listing the directory structure of the dataset, then proceed to load and inspect some key CSV files. Basic statistics and visualizations will be generated to understand the data better.\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "1. [Accessing the Kaggle Dataset](#accessing-the-kaggle-dataset)\n",
    "2. [Data Exploration](#data-exploration)\n",
    "   - [Segmentation of Dataset Population by the Distribution of Their Variables](#segmentation-of-dataset-population-by-the-distribution-of-their-variables)\n",
    "3. [Machine Learning](#machine-learning)\n",
    "\n",
    "# Accessing the Kaggle Dataset\n",
    "[Back to Table of Contents](#table-of-contents)\n",
    "<!-- Content for Accessing the Kaggle Dataset -->\n",
    "\n",
    "This repository includes a GitHub Actions workflow that continuously tests the process of connecting to the Kaggle API, using github secrets to pass username and key securely as environment variables to the automated workflow, and then checks if the files are correctly downloaded and unzipped, if so, a CSV file is imported and printed using pandas.\n",
    "\n",
    "In this research notebook we will use an analogue operation to gather the dataset, but instead of using secrets, the machine that runs this notebook looks for the kaggle.json file that should be in the .kaggle/ folder, to authenticate and download the file. (This file should be included in the .gitignore file, so it's not commited/shared).\n",
    "\n",
    "We can make sure all libraries for this step are imported:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # to create the directory\n",
    "import kaggle # To download the dataset\n",
    "import zipfile # To extract the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "And can create the data directory that will hold the dataset: `../data/`, by using the `makedirs` function from the `os` module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure the data directory exists\n",
    "data_dir = '../data/'\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, since in the `~/.kaggle/` folder we have saved our kaggle credentials (mine are not commited to this repository) the `kaggle.api.dataset_download_files` function uses it automatically to authenticate with the api. \n",
    "\n",
    "Once we define the dataset identifier as `dataset = 'arashnic/fitbit'` the function accesses to the dataset and downloads it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Use Kaggle API to download the dataset\n",
    "dataset = 'arashnic/fitbit'  # The dataset identifier on Kaggle\n",
    "kaggle.api.dataset_download_files(dataset, path=data_dir, unzip=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the file has been downloaded, It can be unzipped as follows:\n",
    "\n",
    "* First the paths of where the file is and where it is going to be extracted are defined: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zip_file_path = '../data/fitbit.zip'\n",
    "extract_to_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then, the zip ifle is extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully unzipped the files, we can take a look to what we are dealing with, by printing the structure of the files within the `../data/` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure of ../data:\n",
      "📄 fitbit.zip\n",
      "📁 mturkfitbit_export_4.12.16-5.12.16/\n",
      "    📁 Fitabase Data 4.12.16-5.12.16/\n",
      "        📄 minuteIntensitiesNarrow_merged.csv\n",
      "        📄 minuteStepsWide_merged.csv\n",
      "        📄 dailyActivity_merged.csv\n",
      "        📄 hourlySteps_merged.csv\n",
      "        📄 dailyIntensities_merged.csv\n",
      "        📄 minuteCaloriesWide_merged.csv\n",
      "        📄 hourlyCalories_merged.csv\n",
      "        📄 minuteStepsNarrow_merged.csv\n",
      "        📄 dailyCalories_merged.csv\n",
      "        📄 minuteCaloriesNarrow_merged.csv\n",
      "        📄 weightLogInfo_merged.csv\n",
      "        📄 hourlyIntensities_merged.csv\n",
      "        📄 dailySteps_merged.csv\n",
      "        📄 minuteMETsNarrow_merged.csv\n",
      "        📄 heartrate_seconds_merged.csv\n",
      "        📄 minuteSleep_merged.csv\n",
      "        📄 minuteIntensitiesWide_merged.csv\n",
      "        📄 sleepDay_merged.csv\n",
      "📁 mturkfitbit_export_3.12.16-4.11.16/\n",
      "    📁 Fitabase Data 3.12.16-4.11.16/\n",
      "        📄 minuteIntensitiesNarrow_merged.csv\n",
      "        📄 dailyActivity_merged.csv\n",
      "        📄 hourlySteps_merged.csv\n",
      "        📄 hourlyCalories_merged.csv\n",
      "        📄 minuteStepsNarrow_merged.csv\n",
      "        📄 minuteCaloriesNarrow_merged.csv\n",
      "        📄 weightLogInfo_merged.csv\n",
      "        📄 hourlyIntensities_merged.csv\n",
      "        📄 minuteMETsNarrow_merged.csv\n",
      "        📄 heartrate_seconds_merged.csv\n",
      "        📄 minuteSleep_merged.csv\n"
     ]
    }
   ],
   "source": [
    "# print the structure of the ../data/ directory\n",
    "\n",
    "def print_directory_structure(root_dir, indent=''):\n",
    "    for item in os.listdir(root_dir):\n",
    "        item_path = os.path.join(root_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"{indent}📁 {item}/\")\n",
    "            print_directory_structure(item_path, indent + '    ')\n",
    "        else:\n",
    "            print(f\"{indent}📄 {item}\")\n",
    "\n",
    "# Define the root directory\n",
    "root_directory = '../data'\n",
    "\n",
    "# Print the directory structure\n",
    "print(f\"Directory structure of {root_directory}:\")\n",
    "print_directory_structure(root_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "[Back to Table of Contents](#table-of-contents)\n",
    "<!-- Content for Data Exploration -->\n",
    "## Segmentation of Dataset Population by the Distribution of Their Variables\n",
    "[Back to Table of Contents](#table-of-contents)\n",
    "<!-- Content for Segmentation of Dataset Population by the Distribution of Their Variables -->\n",
    "# Machine Learning\n",
    "[Back to Table of Contents](#table-of-contents)\n",
    "<!-- Content for Machine Learning -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
