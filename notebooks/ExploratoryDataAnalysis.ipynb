{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitbit Data Analysis\n",
    "\n",
    "## Introduction\n",
    "This notebook aims to explore and analyze the Fitbit dataset. The dataset contains various CSV files with information on daily activities, calories burned, heart rate, sleep patterns, and more. We will start by listing the directory structure of the dataset, then proceed to load and inspect some key CSV files. Basic statistics and visualizations will be generated to understand the data better.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Import Libraries](#Import-Libraries)\n",
    "2. [List Directory Structure](#List-Directory-Structure)\n",
    "3. [Read CSV File](#Read-CSV-File)\n",
    "4. [Basic Statistics](#Basic-Statistics)\n",
    "5. [Check Missing Values](#Check-Missing-Values)\n",
    "6. [Visualize Data](#Visualize-Data)\n",
    "7. [Main Execution](#Main-Execution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Kaggle Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository includes a GitHub Actions workflow that continuously tests the process of connecting to the Kaggle API, via github secrets to pass username and key securely as environment variables, and then checks if the files are correctly downloaded, unzipped and imported to python using pandas.\n",
    "\n",
    "I tried to replicate this behavior saving my credentials in a /.env file, but kept getting 502 bad gateway error. \n",
    "\n",
    "So as a solution to this we will do an analogue process using the `requests` library to handle a HTTP request, and `zipfile` to extract the files within the main folder of the dataset.\n",
    "\n",
    "### installing libraries required to download and unzip the data from the kaggle dataset website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tell python where the file is, and download it\n",
    "\n",
    "The dataset is available at the [kaggle website](https://www.kaggle.com/datasets/arashnic/fitbit). \n",
    "\n",
    "We can create the data directory that will hold the dataset: `../data/`, and use the `makedirs` function from the `os` module.\n",
    "\n",
    "Then, since in the `~/../.kaggle/` folder we have saved our kaggle credentials (mine are not commited to this repository) the `kaggle.api.dataset_download_files` function uses it automatically to authenticate with the api, once we define the dataset identifier as `dataset = 'arashnic/fitbit'` : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/arashnic/fitbit\n",
      "Dataset downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kaggle\n",
    "import zipfile\n",
    "\n",
    "# Step 1: Ensure the data directory exists\n",
    "data_dir = '../data/'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Step 2: Use Kaggle API to download the dataset\n",
    "dataset = 'arashnic/fitbit'  # The dataset identifier on Kaggle\n",
    "kaggle.api.dataset_download_files(dataset, path=data_dir, unzip=False)\n",
    "\n",
    "print(\"Dataset downloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can unzip the files using the `zipfile` module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File extracted successfully.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_file_path = '../data/fitbit.zip'\n",
    "extract_to_path = '../data/'\n",
    "\n",
    "# check if the file is indeed a zip file\n",
    "\n",
    "if zipfile.is_zipfile(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to_path)\n",
    "    print(\"File extracted successfully.\")\n",
    "else:\n",
    "    print(\"The file is not a valid ZIP file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully unzipped the files, we can take a look to what we are dealing with, by printing the structure of the files within the `../data/` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure of ../data:\n",
      "📄 fitbit.zip\n",
      "📁 mturkfitbit_export_4.12.16-5.12.16/\n",
      "    📁 Fitabase Data 4.12.16-5.12.16/\n",
      "        📄 minuteIntensitiesNarrow_merged.csv\n",
      "        📄 minuteStepsWide_merged.csv\n",
      "        📄 dailyActivity_merged.csv\n",
      "        📄 hourlySteps_merged.csv\n",
      "        📄 dailyIntensities_merged.csv\n",
      "        📄 minuteCaloriesWide_merged.csv\n",
      "        📄 hourlyCalories_merged.csv\n",
      "        📄 minuteStepsNarrow_merged.csv\n",
      "        📄 dailyCalories_merged.csv\n",
      "        📄 minuteCaloriesNarrow_merged.csv\n",
      "        📄 weightLogInfo_merged.csv\n",
      "        📄 hourlyIntensities_merged.csv\n",
      "        📄 dailySteps_merged.csv\n",
      "        📄 minuteMETsNarrow_merged.csv\n",
      "        📄 heartrate_seconds_merged.csv\n",
      "        📄 minuteSleep_merged.csv\n",
      "        📄 minuteIntensitiesWide_merged.csv\n",
      "        📄 sleepDay_merged.csv\n",
      "📁 mturkfitbit_export_3.12.16-4.11.16/\n",
      "    📁 Fitabase Data 3.12.16-4.11.16/\n",
      "        📄 minuteIntensitiesNarrow_merged.csv\n",
      "        📄 dailyActivity_merged.csv\n",
      "        📄 hourlySteps_merged.csv\n",
      "        📄 hourlyCalories_merged.csv\n",
      "        📄 minuteStepsNarrow_merged.csv\n",
      "        📄 minuteCaloriesNarrow_merged.csv\n",
      "        📄 weightLogInfo_merged.csv\n",
      "        📄 hourlyIntensities_merged.csv\n",
      "        📄 minuteMETsNarrow_merged.csv\n",
      "        📄 heartrate_seconds_merged.csv\n",
      "        📄 minuteSleep_merged.csv\n"
     ]
    }
   ],
   "source": [
    "# print the structure of the ../data/ directory\n",
    "\n",
    "def print_directory_structure(root_dir, indent=''):\n",
    "    for item in os.listdir(root_dir):\n",
    "        item_path = os.path.join(root_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"{indent}📁 {item}/\")\n",
    "            print_directory_structure(item_path, indent + '    ')\n",
    "        else:\n",
    "            print(f\"{indent}📄 {item}\")\n",
    "\n",
    "# Define the root directory\n",
    "root_directory = '../data'\n",
    "\n",
    "# Print the directory structure\n",
    "print(f\"Directory structure of {root_directory}:\")\n",
    "print_directory_structure(root_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
